{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f90225cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import Utils as utils\n",
    "import json\n",
    "import mediapipe as mp\n",
    "import math\n",
    "import gc\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from statistics import mean\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "126b0159",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FRAMES = 32\n",
    "datos_imgs_video = json.load(open('Logs/DataSetInformation/datasetIMGv3.json'))\n",
    "#path_video = '../../Videos/VideosLSECv1/Training/Lsec_01/28.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efc37774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_valores(diccionario):\n",
    "    lista_imgs_video = []\n",
    "    for keys, values in diccionario.items():\n",
    "        lista_imgs = []\n",
    "        for key, value in values.items():\n",
    "            lista_imgs.append(value['count'])\n",
    "        lista_imgs_video.append(lista_imgs)\n",
    "    return lista_imgs_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51035a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del datos_imgs_video['Training']['count']\n",
    "del datos_imgs_video['Validating']['count']\n",
    "#del datos_imgs_video['Testing']['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3dea910",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_imgs_video = obtener_valores(datos_imgs_video['Validating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e0c2d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_imgs_video[19][54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64a7dc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lista_imgs_menos_fps(lista_imgs, max_frames):\n",
    "    lista_imgs_menos_fps = []\n",
    "    for frase in lista_imgs: \n",
    "        for video in frase:\n",
    "            if(video < max_frames):\n",
    "                lista_imgs_menos_fps.append(video)\n",
    "    return lista_imgs_menos_fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2e9e57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_mano(x_keypoints, y_keypoints, image):\n",
    "    if(x_keypoints and y_keypoints):\n",
    "        x_min, x_max, y_min, y_max = utils.find_max_min(x_keypoints,y_keypoints)\n",
    "        hand_image = np.array(cv2.resize(image[y_min:y_max , x_min:x_max], (50,50)))/255\n",
    "        return hand_image\n",
    "    else:\n",
    "        hand_image = np.zeros((50,50,3))\n",
    "        return hand_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c45b1ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_dataset(carpeta_videos, max_imgs_frames, max_imgs):\n",
    "    with mp_holistic.Holistic(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5\n",
    "        ) as holistic:\n",
    "        videos_salida = []\n",
    "        etiquetas = []\n",
    "        for i, frase in enumerate(utils.sorted_alphanumeric(os.listdir(carpeta_videos))):\n",
    "            print(f'Frase: {frase}')\n",
    "            for j, video in enumerate(utils.sorted_alphanumeric(os.listdir(carpeta_videos+frase))):\n",
    "                if(j > max_imgs):\n",
    "                    break\n",
    "                cap = cv2.VideoCapture(carpeta_videos+frase+'/'+video)\n",
    "                pasos = max_imgs_frames[i][j]/MAX_FRAMES\n",
    "                if(pasos < 1):\n",
    "                    pasos = 1\n",
    "                #Preprocesamiento\n",
    "                imagenes_salida = []\n",
    "                id_imagen = 0\n",
    "                while(cap.isOpened()):\n",
    "                    frameId = cap.get(1)\n",
    "                    ret, frame = cap.read()\n",
    "                    if (ret != True):\n",
    "                        if(len(imagenes_salida) < MAX_FRAMES * 2):\n",
    "                            for x in range(MAX_FRAMES * 2 - len(imagenes_salida)):\n",
    "#                                 print(f'Número de videos aumentados: {x}')\n",
    "                                imagenes_salida.append(np.zeros((50,50,3)))\n",
    "                        break\n",
    "                    #Validación de dimensiones\n",
    "                    if(not frame.shape == (360, 480, 3)):\n",
    "                        frame = cv2.resize(frame, (480,360))\n",
    "                    #MediaPipe\n",
    "                    image, results = utils.mediapipe_detection(frame, holistic)\n",
    "                    rh_keypoints_x, rh_keypoints_y, lh_keypoints_x, lh_keypoints_y = utils.extract_keypoints_V3(results)  \n",
    "                    #Tomar imágenes con al menos una mano en cámara\n",
    "                    if(rh_keypoints_x and rh_keypoints_y or lh_keypoints_x and lh_keypoints_y):\n",
    "                        if(len(imagenes_salida) == 0):\n",
    "#                             print(f'Frame ID escogido: {frameId}')\n",
    "                            imagenes_salida.append(obtener_mano(rh_keypoints_x, rh_keypoints_y, image))\n",
    "                            imagenes_salida.append(obtener_mano(lh_keypoints_x, lh_keypoints_y, image))\n",
    "                            id_imagen += pasos\n",
    "                        elif(int(math.floor(id_imagen)) == frameId and len(imagenes_salida) < MAX_FRAMES*2):\n",
    "#                             print(f'Frame ID escogido: {frameId}')\n",
    "                            imagenes_salida.append(obtener_mano(rh_keypoints_x, rh_keypoints_y, image))\n",
    "                            imagenes_salida.append(obtener_mano(lh_keypoints_x, lh_keypoints_y, image))\n",
    "                            id_imagen += pasos\n",
    "                    else:\n",
    "#                         print(f'Frame ID NO escogido: {frameId}')\n",
    "                        id_imagen += 1\n",
    "                cap.release()\n",
    "                #Guardar los conjuntos de imágenes por video y sus etiquetas\n",
    "                videos_salida.append(imagenes_salida)\n",
    "                etiquetas.append(i)\n",
    "        return np.array(videos_salida), to_categorical(np.array(etiquetas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b2147eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase: Lsec_01\n",
      "Frase: Lsec_02\n",
      "Frase: Lsec_03\n",
      "Frase: Lsec_04\n",
      "Frase: Lsec_05\n",
      "Frase: Lsec_06\n",
      "Frase: Lsec_07\n",
      "Frase: Lsec_08\n",
      "Frase: Lsec_09\n",
      "Frase: Lsec_10\n",
      "Frase: Lsec_11\n",
      "Frase: Lsec_12\n",
      "Frase: Lsec_13\n",
      "Frase: Lsec_14\n",
      "Frase: Lsec_15\n",
      "Frase: Lsec_16\n",
      "Frase: Lsec_17\n",
      "Frase: Lsec_18\n",
      "Frase: Lsec_19\n",
      "Frase: Lsec_20\n"
     ]
    }
   ],
   "source": [
    "carpeta_videos = '../../Videos/VideosLSECv2/Training/'\n",
    "x_val, y_val = obtener_dataset(carpeta_videos, lista_imgs_video, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7737cd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('DataSet/v3_64/x_train.npy', x_val)\n",
    "np.save('DataSet/v3_64/y_train.npy', y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfd67f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase: Lsec_01\n",
      "Frase: Lsec_02\n",
      "Frase: Lsec_03\n",
      "Frase: Lsec_04\n",
      "Frase: Lsec_05\n",
      "Frase: Lsec_06\n",
      "Frase: Lsec_07\n",
      "Frase: Lsec_08\n",
      "Frase: Lsec_09\n",
      "Frase: Lsec_10\n",
      "Frase: Lsec_11\n",
      "Frase: Lsec_12\n",
      "Frase: Lsec_13\n",
      "Frase: Lsec_14\n",
      "Frase: Lsec_15\n",
      "Frase: Lsec_16\n",
      "Frase: Lsec_17\n",
      "Frase: Lsec_18\n",
      "Frase: Lsec_19\n",
      "Frase: Lsec_20\n"
     ]
    }
   ],
   "source": [
    "carpeta_videos = '../../Videos/VideosLSECv2/Validating/'\n",
    "x_val, y_val = obtener_dataset(carpeta_videos, lista_imgs_video, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9637903c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('DataSet/v3_64/x_val.npy', x_val)\n",
    "np.save('DataSet/v3_64/y_val.npy', y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d66f81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu1",
   "language": "python",
   "name": "gpu1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
